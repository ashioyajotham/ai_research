{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YyM8RjqQW-qM"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.linen as nn\n",
        "#!pip install distrax\n",
        "import distrax\n",
        "import optax\n",
        "from typing import Sequence, NamedTuple\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "from flax.training.train_state import TrainState\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "siiUaRvGX6dG"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    # Environment\n",
        "    \"ENV_NAME\": \"CartPole-v1\",\n",
        "    \"NUM_ENVS\": 4,\n",
        "\n",
        "    # Training\n",
        "    \"TOTAL_TIMESTAMPS\": 1e6,\n",
        "    \"NUM_STEPS\": 128,\n",
        "    \"NUM_MINIBATCHES\": 4,\n",
        "    \"LEARNING_RATE\": 2.5e-4,\n",
        "\n",
        "    # PPO Parameters\n",
        "    \"GAMMA\": 0.99,\n",
        "    \"GAE_LAMBDA\": 0.95,\n",
        "    \"CLIP_EPS\": 0.2,\n",
        "    \"ENT_COEF\": 0.01,\n",
        "    \"VF_COEF\": 0.5,\n",
        "\n",
        "    # Network\n",
        "    \"ACTIVATION\": \"tanh\",\n",
        "    \"SEED\": 42,\n",
        "}\n",
        "\n",
        "# Derived Configuration\n",
        "config[\"NUM_UPDATES\"] = int(config[\"TOTAL_TIMESTAMPS\"] // (config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]))\n",
        "config[\"MINIBATCH\"] = int((config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"]) // config[\"NUM_MINIBATCHES\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "XUq7aCiwcBFl"
      },
      "outputs": [],
      "source": [
        "class Transition(NamedTuple):\n",
        "  \"\"\"Data structure to store environment transitions\"\"\"\n",
        "  done: jnp.ndarray\n",
        "  action: jnp.ndarray\n",
        "  value: jnp.ndarray\n",
        "  reward: jnp.ndarray\n",
        "  log_prob: jnp.ndarray\n",
        "  obs: jnp.ndarray\n",
        "  info: dict\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "  \"\"\"Actor Critic Network\"\"\"\n",
        "  action_dim: int\n",
        "  activation: str = \"tanh\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    \"\"\"Defines the forward pass of the Actor Critic Network\"\"\"\n",
        "    if self.activation == \"relu\":\n",
        "      activation = nn.relu\n",
        "    else:\n",
        "      activation = nn.tanh\n",
        "\n",
        "    # Shared layers\n",
        "    shared = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
        "    shared = activation(shared)\n",
        "    shared = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(shared)\n",
        "    shared = activation(shared)\n",
        "\n",
        "    # Actor head - decides what action to take (outputs a distribution probability)\n",
        "    actor_mean = nn.Dense(self.action_dim, kernel_init=orthogonal(0.01))(shared)\n",
        "    pi = distrax.Categorical(logits=actor_mean)\n",
        "\n",
        "    # Critic head - estimates how good the current state is (outputs a value score)\n",
        "    value = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(shared)\n",
        "    return pi, jnp.squeeze(value, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OXxURUrhthu"
      },
      "outputs": [],
      "source": [
        "def make_train(config):\n",
        "  \"\"\"Ensure the training function with the given configuration\"\"\"\n",
        "\n",
        "  # Initialize environment\n",
        "  def make_env():\n",
        "    env = gym.make(config[\"ENV_NAME\"])\n",
        "    return env\n",
        "\n",
        "  # For now let us create a simple vectorized environment wrapper\n",
        "  class SimpleVecEnv:\n",
        "    def __init__(self, env_fns):\n",
        "      self.envs = [env_fn() for env_fn in env_fns]\n",
        "      self.num_envs = len(env_fns)\n",
        "      self.observation_space = self.envs[0].observation_space\n",
        "      self.action_space = self.envs[0].action_space\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "      obs = []\n",
        "      infos = []\n",
        "      if seed is None:\n",
        "          seed_list = [None] * self.num_envs\n",
        "      else:\n",
        "          # Assuming seed is a JAX PRNGKey; jax.random.split returns keys of shape (2,)\n",
        "          # Convert each PRNGKey to a Python int seed by taking the first uint32 word.\n",
        "          # This yields a deterministic, easily convertible integer suitable for gym.reset(seed=...).\n",
        "          keys = jax.random.split(seed, self.num_envs)\n",
        "          seed_list = [int(k[0]) for k in keys]\n",
        "\n",
        "      for i, env in enumerate(self.envs):\n",
        "        if seed_list[i] is not None:\n",
        "          o, info = env.reset(seed=seed_list[i])\n",
        "        else:\n",
        "          o, info = env.reset()\n",
        "        obs.append(o)\n",
        "        infos.append(info)\n",
        "      return np.array(obs), infos\n",
        "\n",
        "    def step(self, actions):\n",
        "      obs, rewards, dones, truncated, infos = [], [], [], [], []\n",
        "      # Ensure actions is an iterable of scalars (one per env)\n",
        "      actions = np.asarray(actions)\n",
        "      for env, action in zip(self.envs, actions):\n",
        "        # action may be a 0-d array, a numpy scalar, or a JAX array; coerce to Python int\n",
        "        a = int(np.array(action).item())\n",
        "        o, r, d, t, info = env.step(a)\n",
        "        if d or t:\n",
        "          # reset the env immediately and use the new observation\n",
        "          o, _ = env.reset()\n",
        "        obs.append(o)\n",
        "        rewards.append(r)\n",
        "        dones.append(d or t)\n",
        "        truncated.append(t)\n",
        "        infos.append(info)\n",
        "\n",
        "      return np.array(obs), np.array(rewards), np.array(dones), np.array(truncated), infos\n",
        "\n",
        "\n",
        "  # Create vectorized environment\n",
        "  env = SimpleVecEnv([make_env for _ in range(config[\"NUM_ENVS\"])])\n",
        "\n",
        "  # Get environment specs\n",
        "  obs_shape = env.observation_space.shape\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  def linear_schedule(count):\n",
        "    \"\"\"Linear learning rate schedule\"\"\"\n",
        "    frac = 1.0 - (count / config[\"TOTAL_TIMESTAMPS\"])\n",
        "    return config[\"LEARNING_RATE\"] * frac\n",
        "\n",
        "\n",
        "  def train(rng):\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    print(\"Starting training inside train function...\")\n",
        "\n",
        "    # Initialize network\n",
        "    network = ActorCritic(action_dim, config[\"ACTIVATION\"])\n",
        "\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    init_x = jnp.zeros(obs_shape)\n",
        "    network_params = network.init(_rng, init_x)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    tx = optax.chain(\n",
        "        optax.clip_by_global_norm(0.5),\n",
        "        optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
        "    )\n",
        "    train_state = TrainState.create(\n",
        "        apply_fn=network.apply,\n",
        "        params=network_params,\n",
        "        tx=tx,\n",
        "    )\n",
        "\n",
        "    # Environment step function\n",
        "    def env_step(runner_state, unused):\n",
        "      train_state, env_state, last_obs, rng = runner_state\n",
        "\n",
        "      # Select action\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      pi, value = network.apply(train_state.params, last_obs)\n",
        "      action = pi.sample(seed=_rng)\n",
        "      log_prob = pi.log_prob(action)\n",
        "\n",
        "      # Environment step\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "\n",
        "      # For now, we will use Numpy environment\n",
        "      obs, rewards, dones, truncated, infos = env.step(np.array(action))\n",
        "\n",
        "      transition = Transition(\n",
        "          done = jnp.array(dones),\n",
        "          action=action,\n",
        "          value=value,\n",
        "          reward=jnp.array(rewards),\n",
        "          log_prob=log_prob,\n",
        "          obs=last_obs,\n",
        "          info=infos)\n",
        "\n",
        "      runner_state = (train_state, env_state, jnp.array(obs), rng)\n",
        "      return runner_state, transition\n",
        "\n",
        "    def calculate_gae(traj_batch, last_val):\n",
        "      \"\"\"Calculate Generalized Advantage Estimation\"\"\"\n",
        "      def get_advantages(gae_and_next_value, transition):\n",
        "        gae, next_value = gae_and_next_value\n",
        "        done, value, reward = transition.done, transition.value, transition.reward\n",
        "\n",
        "        delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
        "        gae = delta + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1-done) * gae\n",
        "\n",
        "        return(gae, value), gae\n",
        "\n",
        "      _, advantages = jax.lax.scan(\n",
        "          get_advantages,\n",
        "          (jnp.zeros_like(last_val), last_val),\n",
        "          traj_batch,\n",
        "          reverse=True\n",
        "      )\n",
        "      return advantages, advantages + traj_batch.value\n",
        "\n",
        "    def update_echo(update_state, unused):\n",
        "      \"\"\"Update function for one epoch\"\"\"\n",
        "      def update_minibatch(train_state, batch_info):\n",
        "        \"\"\"Update function for one minibatch\"\"\"\n",
        "        traj_batch, advantages, targets = batch_info\n",
        "\n",
        "        def loss_fn(params):\n",
        "          # Forward pass\n",
        "          pi, value = network.apply(params, traj_batch.obs)\n",
        "          log_prob = pi.log_prob(traj_batch.action)\n",
        "\n",
        "          # Calculate policy loss\n",
        "          ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
        "          gae = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "          loss_actor1 = ratio * gae\n",
        "          loss_actor2 = jnp.clip(ratio, 1 - config[\"CLIP_EPS\"], 1 + config[\"CLIP_EPS\"]) * gae\n",
        "          loss_actor = -jnp.minimum(loss_actor1, loss_actor2).mean()\n",
        "\n",
        "          # Value loss\n",
        "          value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
        "          value_losses = jnp.square(value - targets)\n",
        "          value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
        "          value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
        "\n",
        "          # Entropy loss\n",
        "          entropy = pi.entropy().mean()\n",
        "          total_loss = (\n",
        "              loss_actor +\n",
        "              config[\"VF_COEF\"] * value_loss -\n",
        "              config[\"ENT_COEF\"] * entropy\n",
        "          )\n",
        "          return total_loss, (loss_actor, value_loss, entropy)\n",
        "\n",
        "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "        total_loss, grads = grad_fn(train_state.params)\n",
        "        train_state = train_state.apply_gradients(grads=grads)\n",
        "        return train_state, total_loss\n",
        "\n",
        "      train_state, traj_batch, advantages, targets, rng = update_state\n",
        "\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      batch_size = config[\"MINIBATCH\"] * config[\"NUM_MINIBATCHES\"]\n",
        "      assert batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ENVS\"], f\"batch_size {batch_size}\" != \"{config['NUM_STEPS'] * config['NUM_ENVS']}\"\n",
        "\n",
        "      permutation = jax.random.permutation(_rng, batch_size)\n",
        "      batch = (traj_batch, advantages, targets)\n",
        "      batch = jax.tree_util.tree_map(lambda x: jnp.reshape(x, (batch_size, ) + x.shape[2:]), batch)\n",
        "\n",
        "      shuffled_batch = jax.tree_util.tree_map(lambda x: jnp.take(x, permutation, axis=0), batch,\n",
        "      )\n",
        "      minibatches = jax.tree_util.tree_map(lambda x: jnp.reshape(x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])), shuffled_batch,\n",
        "      )\n",
        "\n",
        "      train_batch, total_loss = jax.lax.scan(update_minibatch, train_state, minibatches)\n",
        "      update_state = (train_batch, traj_batch, advantages, targets, rng)\n",
        "      return update_state, total_loss\n",
        "\n",
        "    # Initialize environment\n",
        "    rng, _rng = jax.random.split(rng)\n",
        "    reset_rng = _rng # Use the split key directly for the SimpleVecEnv reset\n",
        "    obsv, infos = env.reset(reset_rng)\n",
        "    obsv = jnp.array(obsv)\n",
        "    env_state = obsv # Assuming env_state is the observation\n",
        "\n",
        "    # Training loop\n",
        "    runner_state = (train_state, env_state, obsv, rng)\n",
        "    print(f\"Starting training loop for {config['NUM_UPDATES']} updates...\")\n",
        "    for update in range(config[\"NUM_UPDATES\"]):\n",
        "      # collect rollout\n",
        "      runner_state, traj_batch = jax.lax.scan(\n",
        "          env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
        "      )\n",
        "\n",
        "      # Calculate advantages\n",
        "      train_state, env_state, last_obs, rng = runner_state\n",
        "      _, last_val = network.apply(train_state.params, last_obs)\n",
        "      advantages, targets = calculate_gae(traj_batch, last_val)\n",
        "\n",
        "      # Update network\n",
        "      rng, _rng = jax.random.split(rng)\n",
        "      update_state = (train_state, traj_batch, advantages, targets, rng)\n",
        "      # Assuming NUM_EPOCHS is a config parameter\n",
        "      update_state, loss_info = jax.lax.scan(update_echo, update_state, None, config[\"NUM_EPOCHS\"])\n",
        "      train_state = update_state[0]\n",
        "      runner_state = (train_state, env_state, last_obs, rng)\n",
        "\n",
        "      # Log progress\n",
        "      if update % 10 == 0:\n",
        "        print(f\"Update {update}/{config['NUM_UPDATES']}, Loss: {loss_info[0].mean():.4f}\")\n",
        "\n",
        "    return train_state\n",
        "\n",
        "  return train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4c3a095"
      },
      "source": [
        "The code within the `if __name__ == \"__main__\":` block is not executed when running a notebook cell directly. Moving the code that initiates the training to a separate cell will ensure it runs and you can see the training output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "1a6a00ff",
        "outputId": "9ca0252e-0179-499f-c1ae-a35902a65885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting PPO training...\n",
            "Environment: CartPole-v1\n",
            "Number of environments: 4\n",
            "Total updates: 1953\n",
            "Number of steps: 128\n",
            "Number of minibatches: 4\n",
            "Learning rate: 0.00025\n",
            "Starting training inside train function...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Only scalar arrays can be converted to Python scalars; got arr.ndim=1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1966041759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Learning rate: {config['LEARNING_RATE']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfinal_train_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-544793205.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rng)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mreset_rng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rng\u001b[0m \u001b[0;31m# Use the split key directly for the SimpleVecEnv reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mobsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreset_rng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mobsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0menv_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobsv\u001b[0m \u001b[0;31m# Assuming env_state is the observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-544793205.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0;31m# Assuming seed is a JAX random key array from jax.random.split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0;31m# Convert JAX array keys to integers for gym.Env reset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0mseed_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/array.py\u001b[0m in \u001b[0;36m__int__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__int__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_scalar_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__int__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mcheck_scalar_conversion\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_scalar_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     raise TypeError(\"Only scalar arrays can be converted to Python scalars; \"\n\u001b[0m\u001b[1;32m    869\u001b[0m                     f\"got {arr.ndim=}\")\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Only scalar arrays can be converted to Python scalars; got arr.ndim=1"
          ]
        }
      ],
      "source": [
        "rng = jax.random.PRNGKey(config[\"SEED\"])\n",
        "train_fn = make_train(config)\n",
        "\n",
        "print(\"Starting PPO training...\")\n",
        "print(f\"Environment: {config['ENV_NAME']}\")\n",
        "print(f\"Number of environments: {config['NUM_ENVS']}\")\n",
        "print(f\"Total updates: {config ['NUM_UPDATES']}\")\n",
        "print(f\"Number of steps: {config['NUM_STEPS']}\")\n",
        "print(f\"Number of minibatches: {config['NUM_MINIBATCHES']}\")\n",
        "print(f\"Learning rate: {config['LEARNING_RATE']}\")\n",
        "\n",
        "final_train_state = train_fn(rng)\n",
        "print(\"Training complete!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
